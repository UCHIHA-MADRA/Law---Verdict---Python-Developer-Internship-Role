{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d938d77e",
   "metadata": {},
   "source": [
    "Fixed Rajasthan High Court Judgment Scraper - Testing Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c3a0d1",
   "metadata": {},
   "source": [
    "This notebook contains the fixed version of the Rajasthan HC scraper with improved error handling and captcha solving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c1133",
   "metadata": {},
   "source": [
    "Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637c1a05",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install selenium webdriver-manager pandas opencv-python pillow pytesseract requests numpy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9602de84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (4.35.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: pillow in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (11.3.0)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: requests in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from selenium) (2025.8.3)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from selenium) (4.14.1)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from trio~=0.30.0->selenium) (2.0.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.23)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\prabh\\desktop\\law & verdict - python developer internship role\\venv\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install selenium pandas opencv-python pillow pytesseract requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93395692",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import the fixed scraper class\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Copy the fixed scraper code here or import from the Python file\n",
    "exec(open('rajasthan_hc_scraper_fixed.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf00368",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1f3321",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Install Chrome and ChromeDriver for Colab\n",
    "!apt-get update\n",
    "!apt-get install -y chromium-browser chromium-chromedriver\n",
    "!apt-get install -y tesseract-ocr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ace11a",
   "metadata": {},
   "source": [
    "Quick Test Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378a67e",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the fixed scraper\n",
    "print(\"Initializing Fixed Rajasthan HC Scraper...\")\n",
    "scraper = FixedRajasthanHCScraper(download_dir=\"test_judgments\")\n",
    "\n",
    "# Test with a smaller date range first (last 3 days)\n",
    "print(\"\\nRunning test scrape for last 3 days...\")\n",
    "test_judgments = scraper.run_incremental_scrape(days_back=3)\n",
    "\n",
    "print(f\"\\nTest completed! Found {len(test_judgments)} judgments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f78e7",
   "metadata": {},
   "source": [
    "Check Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30397d",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Display results\n",
    "if os.path.exists(\"test_judgments/judgments.csv\"):\n",
    "    df = pd.read_csv(\"test_judgments/judgments.csv\")\n",
    "    print(f\"Total judgments in database: {len(df)}\")\n",
    "    print(\"\\nColumns:\", list(df.columns))\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        print(\"\\nSample data:\")\n",
    "        display(df.head())\n",
    "        \n",
    "        # Check download statistics\n",
    "        if 'download_status' in df.columns:\n",
    "            print(\"\\nDownload Status:\")\n",
    "            print(df['download_status'].value_counts())\n",
    "else:\n",
    "    print(\"No CSV file found. Check if scraping was successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6e1ca",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "Manual Captcha Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e73d8f",
   "metadata": {},
   "source": [
    "# If you need to test captcha solving manually\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Display a captcha image if one was saved\n",
    "if os.path.exists(\"manual_captcha.png\"):\n",
    "    print(\"Captcha image:\")\n",
    "    display(Image(\"manual_captcha.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c55d24",
   "metadata": {},
   "source": [
    "File Structure Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c80ef",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Check downloaded files\n",
    "import glob\n",
    "\n",
    "base_dir = \"test_judgments\"\n",
    "if os.path.exists(base_dir):\n",
    "    print(f\"Files in {base_dir}:\")\n",
    "    for file in glob.glob(f\"{base_dir}/*\"):\n",
    "        if os.path.isfile(file):\n",
    "            size = os.path.getsize(file)\n",
    "            print(f\"  📄 {os.path.basename(file)} ({size:,} bytes)\")\n",
    "        else:\n",
    "            print(f\"  📁 {os.path.basename(file)}/\")\n",
    "    \n",
    "    # Check PDFs\n",
    "    pdf_dir = f\"{base_dir}/pdfs\"\n",
    "    if os.path.exists(pdf_dir):\n",
    "        pdf_files = glob.glob(f\"{pdf_dir}/*.pdf\")\n",
    "        print(f\"\\nPDFs downloaded ({len(pdf_files)} files):\")\n",
    "        for pdf in pdf_files[:10]:  # Show first 10\n",
    "            size = os.path.getsize(pdf)\n",
    "            print(f\"  📄 {os.path.basename(pdf)} ({size:,} bytes)\")\n",
    "        if len(pdf_files) > 10:\n",
    "            print(f\"  ... and {len(pdf_files) - 10} more PDFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead80972",
   "metadata": {},
   "source": [
    "Custom Date Range Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70634c",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Test with a custom date range\n",
    "from_date = \"01/09/2025\"  # DD/MM/YYYY\n",
    "to_date = \"12/09/2025\"\n",
    "\n",
    "print(f\"Testing custom date range: {from_date} to {to_date}\")\n",
    "custom_judgments = scraper.scrape_judgments(from_date, to_date)\n",
    "\n",
    "print(f\"Found {len(custom_judgments)} judgments in custom range\")\n",
    "\n",
    "if custom_judgments:\n",
    "    # Save results\n",
    "    scraper.save_to_csv(custom_judgments)\n",
    "    scraper.downloaded_judgments[\"last_run_date\"] = datetime.now().isoformat()\n",
    "    scraper.save_state()\n",
    "    print(\"Custom results saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae66ec5",
   "metadata": {},
   "source": [
    "Bonus: SCI Captcha Solver Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7365807e",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize and test the Supreme Court captcha solver\n",
    "print(\"Initializing SCI Captcha Solver...\")\n",
    "sci_solver = SCICaptchaSolver()\n",
    "\n",
    "# If you have a test captcha image, uncomment and use:\n",
    "# test_result = sci_solver.solve_sci_captcha(\"test_captcha.png\")\n",
    "# print(f\"SCI Captcha result: {test_result}\")\n",
    "\n",
    "print(\"SCI Captcha solver ready for use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f3b8f4",
   "metadata": {},
   "source": [
    "Troubleshooting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeff863",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Check if all dependencies are working\n",
    "def test_dependencies():\n",
    "    try:\n",
    "        import selenium\n",
    "        print(f\"✅ Selenium: {selenium.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"❌ Selenium not found\")\n",
    "    \n",
    "    try:\n",
    "        import cv2\n",
    "        print(f\"✅ OpenCV: {cv2.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"❌ OpenCV not found\")\n",
    "    \n",
    "    try:\n",
    "        import pytesseract\n",
    "        print(\"✅ Tesseract: Available\")\n",
    "    except ImportError:\n",
    "        print(\"⚠️ Tesseract not found (manual captcha input required)\")\n",
    "    \n",
    "    try:\n",
    "        from webdriver_manager.chrome import ChromeDriverManager\n",
    "        print(\"✅ WebDriver Manager: Available\")\n",
    "    except ImportError:\n",
    "        print(\"❌ WebDriver Manager not found\")\n",
    "\n",
    "test_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68101882",
   "metadata": {},
   "source": [
    "Debug Mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ee2fa",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Run in debug mode (non-headless) to see what's happening\n",
    "print(\"Running in debug mode (browser visible)...\")\n",
    "\n",
    "# Create a debug version of the scraper\n",
    "debug_scraper = FixedRajasthanHCScraper(download_dir=\"debug_judgments\")\n",
    "\n",
    "# Modify chrome options to be non-headless\n",
    "debug_scraper.chrome_options = Options()\n",
    "debug_scraper.chrome_options.add_argument(\"--no-sandbox\")\n",
    "debug_scraper.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "debug_scraper.chrome_options.add_argument(\"--window-size=1280,720\")\n",
    "\n",
    "# Run a small test\n",
    "debug_judgments = debug_scraper.run_incremental_scrape(days_back=1)\n",
    "print(f\"Debug run completed: {len(debug_judgments)} judgments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f949697",
   "metadata": {},
   "source": [
    "Error Recovery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ece4e4",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# If the scraper gets stuck, you can reset the state\n",
    "def reset_scraper_state():\n",
    "    state_file = \"test_judgments/scraper_state.json\"\n",
    "    if os.path.exists(state_file):\n",
    "        os.remove(state_file)\n",
    "        print(\"Scraper state reset\")\n",
    "    else:\n",
    "        print(\"No state file found\")\n",
    "\n",
    "# Uncomment to reset:\n",
    "# reset_scraper_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609c140c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import the scraper class (copy the main script into a file first)\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a887e9ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rajasthan High Court Judgment Scraper\n",
      "==================================================\n",
      "Running incremental scrape from 01/09/2025 to 11/09/2025\n",
      "Scraping judgments from 01/09/2025 to 11/09/2025\n",
      "Error filling form: 'WebDriverWait' object has no attribute 'wait'\n",
      "Scraping completed. Downloaded 0 new judgments.\n",
      "\n",
      "✅ Successfully processed 0 judgments\n",
      "📁 Files saved in: rajasthan_hc_judgments\n",
      "📄 CSV file: rajasthan_hc_judgments\\judgments.csv\n",
      "📚 PDFs saved in: rajasthan_hc_judgments\\pdfs\n",
      "\n",
      "==================================================\n",
      "Supreme Court Captcha Solver (Bonus)\n",
      "==================================================\n",
      "SCI Captcha solver initialized. Use sci_solver.solve_sci_captcha('path_to_captcha.png')\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Rajasthan High Court Judgment Scraper\n",
    "Incrementally downloads judgments from https://hcraj.nic.in/cishcraj-jdp/JudgementFilters/\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import re\n",
    "\n",
    "class RajasthanHCJudgmentScraper:\n",
    "    def __init__(self, download_dir: str = \"rajasthan_hc_judgments\"):\n",
    "        self.base_url = \"https://hcraj.nic.in/cishcraj-jdp/JudgementFilters/\"\n",
    "        self.download_dir = Path(download_dir)\n",
    "        self.pdf_dir = self.download_dir / \"pdfs\"\n",
    "        self.csv_file = self.download_dir / \"judgments.csv\"\n",
    "        self.state_file = self.download_dir / \"scraper_state.json\"\n",
    "        \n",
    "        # Create directories\n",
    "        self.download_dir.mkdir(exist_ok=True)\n",
    "        self.pdf_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Initialize state\n",
    "        self.downloaded_judgments = self.load_state()\n",
    "        \n",
    "        # Setup Chrome options\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")  # Remove for debugging\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.chrome_options.add_experimental_option(\"prefs\", {\n",
    "            \"download.default_directory\": str(self.pdf_dir.absolute()),\n",
    "            \"download.prompt_for_download\": False,\n",
    "            \"plugins.always_open_pdf_externally\": True\n",
    "        })\n",
    "        \n",
    "    def load_state(self) -> Dict:\n",
    "        \"\"\"Load previously downloaded judgment IDs and metadata\"\"\"\n",
    "        if self.state_file.exists():\n",
    "            with open(self.state_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {\"downloaded_ids\": set(), \"last_run_date\": None}\n",
    "    \n",
    "    def save_state(self):\n",
    "        \"\"\"Save current state to file\"\"\"\n",
    "        state_to_save = {\n",
    "            \"downloaded_ids\": list(self.downloaded_judgments[\"downloaded_ids\"]),\n",
    "            \"last_run_date\": self.downloaded_judgments[\"last_run_date\"]\n",
    "        }\n",
    "        with open(self.state_file, 'w') as f:\n",
    "            json.dump(state_to_save, f, indent=2)\n",
    "    \n",
    "    def generate_judgment_id(self, judgment_data: Dict) -> str:\n",
    "        \"\"\"Generate unique ID for judgment based on key fields\"\"\"\n",
    "        id_string = f\"{judgment_data.get('case_number', '')}_{judgment_data.get('judgment_date', '')}_{judgment_data.get('judge_name', '')}\"\n",
    "        return hashlib.md5(id_string.encode()).hexdigest()\n",
    "    \n",
    "    def solve_captcha(self, captcha_image_element) -> str:\n",
    "        \"\"\"\n",
    "        Simple captcha solver using OCR\n",
    "        This is a basic implementation - may need refinement based on captcha complexity\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Take screenshot of captcha\n",
    "            captcha_image_element.screenshot(\"temp_captcha.png\")\n",
    "            \n",
    "            # Load and preprocess image\n",
    "            img = cv2.imread(\"temp_captcha.png\")\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Apply preprocessing to improve OCR accuracy\n",
    "            # Remove noise\n",
    "            denoised = cv2.medianBlur(gray, 3)\n",
    "            \n",
    "            # Threshold to get binary image\n",
    "            _, thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "            # OCR with specific configuration for captcha\n",
    "            custom_config = r'--oem 3 --psm 7 -c tesseract_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
    "            captcha_text = pytesseract.image_to_string(thresh, config=custom_config).strip()\n",
    "            \n",
    "            # Clean up\n",
    "            os.remove(\"temp_captcha.png\")\n",
    "            \n",
    "            # Basic validation - captchas are usually 4-6 characters\n",
    "            if len(captcha_text) >= 3 and captcha_text.isalnum():\n",
    "                return captcha_text\n",
    "            else:\n",
    "                return \"\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error solving captcha: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def setup_driver(self) -> webdriver.Chrome:\n",
    "        \"\"\"Initialize Chrome WebDriver\"\"\"\n",
    "        return webdriver.Chrome(options=self.chrome_options)\n",
    "    \n",
    "    def fill_form_and_submit(self, driver: webdriver.Chrome, from_date: str, to_date: str, max_retries: int = 3) -> bool:\n",
    "        \"\"\"Fill the judgment search form and submit\"\"\"\n",
    "        try:\n",
    "            # Wait for page to load\n",
    "            WebDriverWait(driver, 10).wait(\n",
    "                EC.presence_of_element_located((By.NAME, \"fromDate\"))\n",
    "            )\n",
    "            \n",
    "            # Fill from date\n",
    "            from_date_field = driver.find_element(By.NAME, \"fromDate\")\n",
    "            from_date_field.clear()\n",
    "            from_date_field.send_keys(from_date)\n",
    "            \n",
    "            # Fill to date\n",
    "            to_date_field = driver.find_element(By.NAME, \"toDate\")\n",
    "            to_date_field.clear()\n",
    "            to_date_field.send_keys(to_date)\n",
    "            \n",
    "            # Set reportable judgment to YES\n",
    "            try:\n",
    "                reportable_dropdown = Select(driver.find_element(By.NAME, \"reportable\"))\n",
    "                reportable_dropdown.select_by_value(\"Y\")\n",
    "            except:\n",
    "                print(\"Could not find reportable judgment dropdown\")\n",
    "            \n",
    "            # Handle captcha with retries\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    captcha_img = driver.find_element(By.XPATH, \"//img[contains(@src, 'captcha')]\")\n",
    "                    captcha_text = self.solve_captcha(captcha_img)\n",
    "                    \n",
    "                    if captcha_text:\n",
    "                        captcha_field = driver.find_element(By.NAME, \"captcha\")\n",
    "                        captcha_field.clear()\n",
    "                        captcha_field.send_keys(captcha_text)\n",
    "                        \n",
    "                        # Submit form\n",
    "                        submit_btn = driver.find_element(By.XPATH, \"//input[@type='submit' or @value='Search']\")\n",
    "                        submit_btn.click()\n",
    "                        \n",
    "                        # Check if submission was successful\n",
    "                        time.sleep(3)\n",
    "                        if \"No records found\" not in driver.page_source and \"Invalid captcha\" not in driver.page_source.lower():\n",
    "                            return True\n",
    "                        else:\n",
    "                            print(f\"Captcha attempt {attempt + 1} failed, retrying...\")\n",
    "                            driver.refresh()\n",
    "                            time.sleep(2)\n",
    "                    else:\n",
    "                        print(f\"Could not solve captcha, attempt {attempt + 1}\")\n",
    "                        driver.refresh()\n",
    "                        time.sleep(2)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in captcha attempt {attempt + 1}: {e}\")\n",
    "                    driver.refresh()\n",
    "                    time.sleep(2)\n",
    "            \n",
    "            print(\"Failed to solve captcha after all attempts\")\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error filling form: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_judgment_data(self, driver: webdriver.Chrome) -> List[Dict]:\n",
    "        \"\"\"Extract judgment data from results table\"\"\"\n",
    "        judgments = []\n",
    "        \n",
    "        try:\n",
    "            # Wait for results table\n",
    "            WebDriverWait(driver, 10).wait(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
    "            )\n",
    "            \n",
    "            # Find the results table\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            results_table = None\n",
    "            \n",
    "            for table in tables:\n",
    "                if \"S.No.\" in table.text or \"Case Number\" in table.text:\n",
    "                    results_table = table\n",
    "                    break\n",
    "            \n",
    "            if not results_table:\n",
    "                print(\"Could not find results table\")\n",
    "                return judgments\n",
    "            \n",
    "            # Extract table headers\n",
    "            headers = []\n",
    "            header_row = results_table.find_element(By.TAG_NAME, \"tr\")\n",
    "            for th in header_row.find_elements(By.TAG_NAME, \"th\"):\n",
    "                headers.append(th.text.strip())\n",
    "            \n",
    "            # Extract data rows\n",
    "            rows = results_table.find_elements(By.TAG_NAME, \"tr\")[1:]  # Skip header\n",
    "            \n",
    "            for row in rows:\n",
    "                cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                if len(cells) < len(headers):\n",
    "                    continue\n",
    "                \n",
    "                judgment_data = {}\n",
    "                for i, cell in enumerate(cells):\n",
    "                    if i < len(headers):\n",
    "                        judgment_data[headers[i]] = cell.text.strip()\n",
    "                \n",
    "                # Look for PDF download link\n",
    "                pdf_links = row.find_elements(By.XPATH, \".//a[contains(@href, '.pdf') or contains(text(), 'View') or contains(text(), 'Download')]\")\n",
    "                if pdf_links:\n",
    "                    judgment_data['pdf_url'] = pdf_links[0].get_attribute('href')\n",
    "                else:\n",
    "                    judgment_data['pdf_url'] = \"\"\n",
    "                \n",
    "                judgments.append(judgment_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting judgment data: {e}\")\n",
    "        \n",
    "        return judgments\n",
    "    \n",
    "    def download_pdf(self, url: str, filename: str) -> bool:\n",
    "        \"\"\"Download PDF from URL\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            pdf_path = self.pdf_dir / filename\n",
    "            with open(pdf_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading PDF {filename}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_pdf_filename(self, judgment_data: Dict) -> str:\n",
    "        \"\"\"Generate safe filename for PDF\"\"\"\n",
    "        case_num = judgment_data.get('Case Number', 'Unknown').replace('/', '_').replace('\\\\', '_')\n",
    "        date = judgment_data.get('Judgment Date', '').replace('/', '_')\n",
    "        return f\"{case_num}_{date}.pdf\"\n",
    "    \n",
    "    def scrape_judgments(self, from_date: str, to_date: str) -> List[Dict]:\n",
    "        \"\"\"Main scraping function\"\"\"\n",
    "        print(f\"Scraping judgments from {from_date} to {to_date}\")\n",
    "        \n",
    "        driver = self.setup_driver()\n",
    "        all_judgments = []\n",
    "        \n",
    "        try:\n",
    "            driver.get(self.base_url)\n",
    "            \n",
    "            if self.fill_form_and_submit(driver, from_date, to_date):\n",
    "                judgments = self.extract_judgment_data(driver)\n",
    "                \n",
    "                for judgment in judgments:\n",
    "                    judgment_id = self.generate_judgment_id(judgment)\n",
    "                    \n",
    "                    # Skip if already downloaded\n",
    "                    if judgment_id in self.downloaded_judgments[\"downloaded_ids\"]:\n",
    "                        print(f\"Skipping already downloaded judgment: {judgment.get('Case Number', 'Unknown')}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Download PDF if URL exists\n",
    "                    pdf_filename = \"\"\n",
    "                    if judgment.get('pdf_url'):\n",
    "                        pdf_filename = self.generate_pdf_filename(judgment)\n",
    "                        if self.download_pdf(judgment['pdf_url'], pdf_filename):\n",
    "                            judgment['pdf_filename'] = pdf_filename\n",
    "                            print(f\"Downloaded: {pdf_filename}\")\n",
    "                        else:\n",
    "                            judgment['pdf_filename'] = \"Download_Failed\"\n",
    "                    else:\n",
    "                        judgment['pdf_filename'] = \"No_PDF_URL\"\n",
    "                    \n",
    "                    # Mark as downloaded\n",
    "                    self.downloaded_judgments[\"downloaded_ids\"].add(judgment_id)\n",
    "                    all_judgments.append(judgment)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error during scraping: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            driver.quit()\n",
    "        \n",
    "        return all_judgments\n",
    "    \n",
    "    def save_to_csv(self, judgments: List[Dict]):\n",
    "        \"\"\"Save judgments to CSV file\"\"\"\n",
    "        if not judgments:\n",
    "            print(\"No new judgments to save\")\n",
    "            return\n",
    "        \n",
    "        # Load existing data if CSV exists\n",
    "        existing_df = pd.DataFrame()\n",
    "        if self.csv_file.exists():\n",
    "            try:\n",
    "                existing_df = pd.read_csv(self.csv_file)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Create new DataFrame\n",
    "        new_df = pd.DataFrame(judgments)\n",
    "        \n",
    "        # Combine and save\n",
    "        if not existing_df.empty:\n",
    "            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        else:\n",
    "            combined_df = new_df\n",
    "        \n",
    "        combined_df.to_csv(self.csv_file, index=False)\n",
    "        print(f\"Saved {len(judgments)} new judgments to {self.csv_file}\")\n",
    "    \n",
    "    def run_incremental_scrape(self):\n",
    "        \"\"\"Run incremental scraping for last 10 days\"\"\"\n",
    "        today = datetime.now()\n",
    "        from_date_obj = today - timedelta(days=10)\n",
    "        \n",
    "        from_date = from_date_obj.strftime(\"%d/%m/%Y\")\n",
    "        to_date = today.strftime(\"%d/%m/%Y\")\n",
    "        \n",
    "        print(f\"Running incremental scrape from {from_date} to {to_date}\")\n",
    "        \n",
    "        judgments = self.scrape_judgments(from_date, to_date)\n",
    "        \n",
    "        if judgments:\n",
    "            self.save_to_csv(judgments)\n",
    "        \n",
    "        # Update state\n",
    "        self.downloaded_judgments[\"last_run_date\"] = today.isoformat()\n",
    "        self.save_state()\n",
    "        \n",
    "        print(f\"Scraping completed. Downloaded {len(judgments)} new judgments.\")\n",
    "        return judgments\n",
    "\n",
    "# Bonus: Supreme Court Captcha Solver\n",
    "class SCICaptchaSolver:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def preprocess_captcha_image(self, image_path: str) -> np.ndarray:\n",
    "        \"\"\"Preprocess captcha image for better OCR\"\"\"\n",
    "        # Load image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            img = np.array(Image.open(image_path))\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        if len(img.shape) == 3:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray = img\n",
    "        \n",
    "        # Resize image for better OCR\n",
    "        height, width = gray.shape\n",
    "        if height < 50:\n",
    "            scale_factor = 50 / height\n",
    "            new_width = int(width * scale_factor)\n",
    "            gray = cv2.resize(gray, (new_width, 50))\n",
    "        \n",
    "        # Apply Gaussian blur to reduce noise\n",
    "        blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
    "        \n",
    "        # Apply threshold\n",
    "        _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Morphological operations to clean up\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        cleaned = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def solve_sci_captcha(self, captcha_image_path: str) -> str:\n",
    "        \"\"\"Solve Supreme Court captcha\"\"\"\n",
    "        try:\n",
    "            processed_img = self.preprocess_captcha_image(captcha_image_path)\n",
    "            \n",
    "            # OCR configuration for Supreme Court captcha\n",
    "            custom_config = r'--oem 3 --psm 8 -c tesseract_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
    "            \n",
    "            # Try OCR\n",
    "            captcha_text = pytesseract.image_to_string(processed_img, config=custom_config).strip()\n",
    "            \n",
    "            # Clean the result\n",
    "            captcha_text = re.sub(r'[^A-Z0-9]', '', captcha_text.upper())\n",
    "            \n",
    "            return captcha_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error solving SCI captcha: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the scraper\"\"\"\n",
    "    print(\"Rajasthan High Court Judgment Scraper\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = RajasthanHCJudgmentScraper()\n",
    "    \n",
    "    # Run incremental scrape\n",
    "    try:\n",
    "        judgments = scraper.run_incremental_scrape()\n",
    "        print(f\"\\n✅ Successfully processed {len(judgments)} judgments\")\n",
    "        print(f\"📁 Files saved in: {scraper.download_dir}\")\n",
    "        print(f\"📄 CSV file: {scraper.csv_file}\")\n",
    "        print(f\"📚 PDFs saved in: {scraper.pdf_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error running scraper: {e}\")\n",
    "        \n",
    "    # Bonus: SCI Captcha solver demo\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Supreme Court Captcha Solver (Bonus)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    sci_solver = SCICaptchaSolver()\n",
    "    print(\"SCI Captcha solver initialized. Use sci_solver.solve_sci_captcha('path_to_captcha.png')\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59fce4c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing Rajasthan HC Scraper...\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize the scraper\n",
    "print(\"🚀 Initializing Rajasthan HC Scraper...\")\n",
    "scraper = RajasthanHCJudgmentScraper(download_dir=\"rajasthan_hc_judgments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4530ede1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📥 Running incremental scrape for last 10 days...\n",
      "Running incremental scrape from 01/09/2025 to 11/09/2025\n",
      "Scraping judgments from 01/09/2025 to 11/09/2025\n",
      "Error filling form: 'WebDriverWait' object has no attribute 'wait'\n",
      "Scraping completed. Downloaded 0 new judgments.\n",
      "✅ Successfully downloaded 0 new judgments\n"
     ]
    }
   ],
   "source": [
    "# 2. Run incremental scraping (default: last 10 days)\n",
    "print(\"\\n📥 Running incremental scrape for last 10 days...\")\n",
    "try:\n",
    "    judgments = scraper.run_incremental_scrape()\n",
    "    print(f\"✅ Successfully downloaded {len(judgments)} new judgments\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c77f8f59",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No CSV file found. Scraping may have failed.\n"
     ]
    }
   ],
   "source": [
    "# 3. Display results\n",
    "if os.path.exists(\"rajasthan_hc_judgments/judgments.csv\"):\n",
    "    df = pd.read_csv(\"rajasthan_hc_judgments/judgments.csv\")\n",
    "    print(f\"\\n📊 Total judgments in database: {len(df)}\")\n",
    "    print(\"\\n📋 Sample data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Display statistics\n",
    "    print(\"\\n📈 Statistics:\")\n",
    "    print(f\"• Total judgments: {len(df)}\")\n",
    "    print(f\"• PDFs downloaded: {len(df[df['pdf_filename'].str.contains('.pdf', na=False)])}\")\n",
    "    print(f\"• Failed downloads: {len(df[df['pdf_filename'] == 'Download_Failed'])}\")\n",
    "    print(f\"• No PDF URL: {len(df[df['pdf_filename'] == 'No_PDF_URL'])}\")\n",
    "else:\n",
    "    print(\"❌ No CSV file found. Scraping may have failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8ecb14b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗓️ Custom date range example:\n",
      "Scraping from 01/09/2024 to 11/09/2024...\n",
      "Scraping judgments from 01/09/2024 to 11/09/2024\n",
      "Error filling form: 'WebDriverWait' object has no attribute 'wait'\n",
      "✅ Found 0 judgments in custom range\n"
     ]
    }
   ],
   "source": [
    "# 4. Custom date range scraping\n",
    "print(\"\\n🗓️ Custom date range example:\")\n",
    "from_date = \"01/09/2024\"  # DD/MM/YYYY format\n",
    "to_date = \"11/09/2024\"\n",
    "\n",
    "print(f\"Scraping from {from_date} to {to_date}...\")\n",
    "try:\n",
    "    custom_judgments = scraper.scrape_judgments(from_date, to_date)\n",
    "    print(f\"✅ Found {len(custom_judgments)} judgments in custom range\")\n",
    "    \n",
    "    if custom_judgments:\n",
    "        scraper.save_to_csv(custom_judgments)\n",
    "        scraper.downloaded_judgments[\"last_run_date\"] = datetime.now().isoformat()\n",
    "        scraper.save_state()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Custom scraping error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4b5f4fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Supreme Court Captcha Solver (Bonus):\n",
      "\n",
      "📁 File structure:\n",
      "\n",
      "📚 PDFs downloaded:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# 5. Bonus: SCI Captcha Solver Demo\n",
    "print(\"\\n🎯 Supreme Court Captcha Solver (Bonus):\")\n",
    "sci_solver = SCICaptchaSolver()\n",
    "\n",
    "# Example usage (you would need to provide an actual captcha image)\n",
    "# captcha_result = sci_solver.solve_sci_captcha(\"captcha_image.png\")\n",
    "# print(f\"Captcha solved: {captcha_result}\")\n",
    "\n",
    "print(\"\\n📁 File structure:\")\n",
    "!ls -la rajasthan_hc_judgments/\n",
    "print(\"\\n📚 PDFs downloaded:\")\n",
    "!ls -la rajasthan_hc_judgments/pdfs/ | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a1002e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Notebook execution completed!\n",
      "\n",
      "📖 How to use:\n",
      "1. The scraper runs incrementally - it remembers what it has downloaded\n",
      "2. Run the scraper daily to get new judgments\n",
      "3. All data is saved in 'rajasthan_hc_judgments/' folder\n",
      "4. CSV contains all metadata, PDFs are in 'pdfs/' subfolder\n",
      "5. State is tracked in 'scraper_state.json'\n",
      "\n",
      "🔧 Troubleshooting Tips:\n",
      "1. If captcha solving fails, the script will retry 3 times\n",
      "2. For Colab, make sure to install all dependencies\n",
      "3. Check internet connection if downloads fail\n",
      "4. Captcha OCR may need fine-tuning based on actual captcha images\n",
      "5. The script handles various table formats automatically\n"
     ]
    }
   ],
   "source": [
    "# 6. Data analysis examples\n",
    "if os.path.exists(\"rajasthan_hc_judgments/judgments.csv\"):\n",
    "    df = pd.read_csv(\"rajasthan_hc_judgments/judgments.csv\")\n",
    "    \n",
    "    print(\"\\n📊 Data Analysis:\")\n",
    "    \n",
    "    # Check for date column variations\n",
    "    date_columns = [col for col in df.columns if 'date' in col.lower()]\n",
    "    print(f\"Date columns found: {date_columns}\")\n",
    "    \n",
    "    # Show unique values in key columns\n",
    "    for col in df.columns[:5]:  # First 5 columns\n",
    "        print(f\"\\n🔍 Column '{col}' - Unique values: {df[col].nunique()}\")\n",
    "        if df[col].nunique() < 10:\n",
    "            print(f\"   Values: {df[col].unique()[:5]}\")\n",
    "\n",
    "print(\"\\n✅ Notebook execution completed!\")\n",
    "print(\"\\n📖 How to use:\")\n",
    "print(\"1. The scraper runs incrementally - it remembers what it has downloaded\")\n",
    "print(\"2. Run the scraper daily to get new judgments\")\n",
    "print(\"3. All data is saved in 'rajasthan_hc_judgments/' folder\")\n",
    "print(\"4. CSV contains all metadata, PDFs are in 'pdfs/' subfolder\")\n",
    "print(\"5. State is tracked in 'scraper_state.json'\")\n",
    "\n",
    "# === TROUBLESHOOTING SECTION ===\n",
    "print(\"\\n🔧 Troubleshooting Tips:\")\n",
    "print(\"1. If captcha solving fails, the script will retry 3 times\")\n",
    "print(\"2. For Colab, make sure to install all dependencies\")\n",
    "print(\"3. Check internet connection if downloads fail\")\n",
    "print(\"4. Captcha OCR may need fine-tuning based on actual captcha images\")\n",
    "print(\"5. The script handles various table formats automatically\")\n",
    "\n",
    "# === MANUAL CAPTCHA FALLBACK ===\n",
    "def manual_run_with_captcha():\n",
    "    \"\"\"\n",
    "    Manual mode where user can input captcha\n",
    "    Use this if automated captcha solving doesn't work\n",
    "    \"\"\"\n",
    "    print(\"\\n🔧 Manual captcha mode available - modify the scraper to accept manual input\")\n",
    "    print(\"Replace the solve_captcha method with manual input for testing\")\n",
    "    \n",
    "    # Example manual captcha input modification:\n",
    "    manual_code = \"\"\"\n",
    "    def solve_captcha_manual(self, captcha_image_element) -> str:\n",
    "        captcha_image_element.screenshot(\"captcha_display.png\")\n",
    "        from IPython.display import Image, display\n",
    "        display(Image(\"captcha_display.png\"))\n",
    "        return input(\"Please enter the captcha: \").strip()\n",
    "    \"\"\"\n",
    "    print(\"Replace solve_captcha method with manual input if needed:\")\n",
    "    print(manual_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69395348",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏛️ Rajasthan High Court Judgment Scraper (Windows Version)\n",
      "============================================================\n",
      "🚀 Initializing scraper...\n",
      "📥 Running incremental scrape for last 10 days...\n",
      "📅 Running incremental scrape from 01/09/2025 to 11/09/2025\n",
      "🔍 Scraping judgments from 01/09/2025 to 11/09/2025\n",
      "🌐 Loading website...\n",
      "Error filling form: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0xa2d2a3+66419]\n",
      "\tGetHandleVerifier [0x0xa2d2e4+66484]\n",
      "\t(No symbol) [0x0x804bd3]\n",
      "\t(No symbol) [0x0x84e958]\n",
      "\t(No symbol) [0x0x84ecfb]\n",
      "\t(No symbol) [0x0x895152]\n",
      "\t(No symbol) [0x0x871064]\n",
      "\t(No symbol) [0x0x8928a1]\n",
      "\t(No symbol) [0x0x870e16]\n",
      "\t(No symbol) [0x0x8425ce]\n",
      "\t(No symbol) [0x0x8434a4]\n",
      "\tGetHandleVerifier [0x0xc75ee3+2461619]\n",
      "\tGetHandleVerifier [0x0xc70f66+2441270]\n",
      "\tGetHandleVerifier [0x0xa56242+234258]\n",
      "\tGetHandleVerifier [0x0xa46208+168664]\n",
      "\tGetHandleVerifier [0x0xa4d1ad+197245]\n",
      "\tGetHandleVerifier [0x0xa355f8+100040]\n",
      "\tGetHandleVerifier [0x0xa35792+100450]\n",
      "\tGetHandleVerifier [0x0xa1f74a+10266]\n",
      "\tBaseThreadInitThunk [0x0x778bfcc9+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x77c082ae+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x77c0827e+238]\n",
      "\n",
      "❌ Failed to submit form\n",
      "✅ Scraping completed. Downloaded 0 new judgments.\n",
      "ℹ️ No new judgments found\n",
      "\n",
      "📁 Files in rajasthan_hc_judgments:\n",
      "  📁 pdfs (0 bytes)\n",
      "  📄 scraper_state.json (78 bytes)\n",
      "\n",
      "📚 PDFs downloaded (0 files):\n"
     ]
    }
   ],
   "source": [
    "# Windows-Compatible Rajasthan High Court Scraper\n",
    "# Fixed version for Windows environment\n",
    "\n",
    "# First, install required packages for Windows\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_windows_dependencies():\n",
    "    \"\"\"Install dependencies for Windows\"\"\"\n",
    "    packages = [\n",
    "        'selenium>=4.15.0',\n",
    "        'pandas>=1.5.0',\n",
    "        'opencv-python>=4.8.0',\n",
    "        'Pillow>=10.0.0',\n",
    "        'pytesseract>=0.3.10',\n",
    "        'requests>=2.31.0',\n",
    "        'numpy>=1.24.0',\n",
    "        'webdriver-manager'  # This will auto-manage ChromeDriver\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "\n",
    "# Uncomment the line below if you need to install dependencies\n",
    "# install_windows_dependencies()\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import pytesseract with error handling\n",
    "try:\n",
    "    import pytesseract\n",
    "    # For Windows, you might need to set the tesseract path\n",
    "    # Uncomment and modify the path below if needed\n",
    "    pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "    TESSERACT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ Tesseract not available. Captcha solving will be disabled.\")\n",
    "    TESSERACT_AVAILABLE = False\n",
    "\n",
    "class RajasthanHCJudgmentScraperWindows:\n",
    "    def __init__(self, download_dir: str = \"rajasthan_hc_judgments\"):\n",
    "        self.base_url = \"https://hcraj.nic.in/cishcraj-jdp/JudgementFilters/\"\n",
    "        self.download_dir = Path(download_dir)\n",
    "        self.pdf_dir = self.download_dir / \"pdfs\"\n",
    "        self.csv_file = self.download_dir / \"judgments.csv\"\n",
    "        self.state_file = self.download_dir / \"scraper_state.json\"\n",
    "        \n",
    "        # Create directories\n",
    "        self.download_dir.mkdir(exist_ok=True)\n",
    "        self.pdf_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Initialize state\n",
    "        self.downloaded_judgments = self.load_state()\n",
    "        \n",
    "        # Setup Chrome options for Windows\n",
    "        self.chrome_options = Options()\n",
    "        self.chrome_options.add_argument(\"--headless\")  # Comment this out to see browser\n",
    "        self.chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        self.chrome_options.add_argument(\"--disable-gpu\")\n",
    "        self.chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        \n",
    "        # Set download preferences\n",
    "        prefs = {\n",
    "            \"download.default_directory\": str(self.pdf_dir.absolute()),\n",
    "            \"download.prompt_for_download\": False,\n",
    "            \"plugins.always_open_pdf_externally\": True,\n",
    "            \"profile.default_content_settings.popups\": 0\n",
    "        }\n",
    "        self.chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "        \n",
    "    def load_state(self) -> Dict:\n",
    "        \"\"\"Load previously downloaded judgment IDs and metadata\"\"\"\n",
    "        if self.state_file.exists():\n",
    "            try:\n",
    "                with open(self.state_file, 'r') as f:\n",
    "                    state = json.load(f)\n",
    "                    # Convert list back to set\n",
    "                    state[\"downloaded_ids\"] = set(state.get(\"downloaded_ids\", []))\n",
    "                    return state\n",
    "            except:\n",
    "                pass\n",
    "        return {\"downloaded_ids\": set(), \"last_run_date\": None}\n",
    "    \n",
    "    def save_state(self):\n",
    "        \"\"\"Save current state to file\"\"\"\n",
    "        state_to_save = {\n",
    "            \"downloaded_ids\": list(self.downloaded_judgments[\"downloaded_ids\"]),\n",
    "            \"last_run_date\": self.downloaded_judgments[\"last_run_date\"]\n",
    "        }\n",
    "        with open(self.state_file, 'w') as f:\n",
    "            json.dump(state_to_save, f, indent=2)\n",
    "    \n",
    "    def generate_judgment_id(self, judgment_data: Dict) -> str:\n",
    "        \"\"\"Generate unique ID for judgment based on key fields\"\"\"\n",
    "        # Use first few keys from the judgment data\n",
    "        keys = list(judgment_data.keys())[:3]  # First 3 columns\n",
    "        id_parts = []\n",
    "        for key in keys:\n",
    "            value = str(judgment_data.get(key, ''))\n",
    "            id_parts.append(value)\n",
    "        \n",
    "        id_string = \"_\".join(id_parts)\n",
    "        return hashlib.md5(id_string.encode()).hexdigest()\n",
    "    \n",
    "    def solve_captcha_manual(self, driver) -> str:\n",
    "        \"\"\"Manual captcha input (fallback when OCR fails)\"\"\"\n",
    "        try:\n",
    "            # Try to find and save captcha image\n",
    "            captcha_imgs = driver.find_elements(By.XPATH, \"//img[contains(@src, 'captcha') or contains(@src, 'Captcha')]\")\n",
    "            if captcha_imgs:\n",
    "                captcha_imgs[0].screenshot(\"captcha_temp.png\")\n",
    "                print(\"🖼️ Captcha image saved as 'captcha_temp.png'\")\n",
    "                print(\"📖 Please open the image and enter the captcha text.\")\n",
    "            \n",
    "            captcha_text = input(\"Enter captcha text (or press Enter to skip): \").strip()\n",
    "            return captcha_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error in manual captcha: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def solve_captcha(self, driver) -> str:\n",
    "        \"\"\"\n",
    "        Solve captcha - try OCR first, fallback to manual\n",
    "        \"\"\"\n",
    "        if not TESSERACT_AVAILABLE:\n",
    "            return self.solve_captcha_manual(driver)\n",
    "        \n",
    "        try:\n",
    "            # Find captcha image\n",
    "            captcha_imgs = driver.find_elements(By.XPATH, \"//img[contains(@src, 'captcha') or contains(@src, 'Captcha')]\")\n",
    "            if not captcha_imgs:\n",
    "                print(\"No captcha image found\")\n",
    "                return \"\"\n",
    "            \n",
    "            # Save captcha image\n",
    "            captcha_imgs[0].screenshot(\"temp_captcha.png\")\n",
    "            \n",
    "            # Load and preprocess image\n",
    "            img = cv2.imread(\"temp_captcha.png\")\n",
    "            if img is None:\n",
    "                return self.solve_captcha_manual(driver)\n",
    "            \n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Apply preprocessing to improve OCR accuracy\n",
    "            denoised = cv2.medianBlur(gray, 3)\n",
    "            _, thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "            # OCR with specific configuration for captcha\n",
    "            custom_config = r'--oem 3 --psm 7 -c tesseract_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
    "            captcha_text = pytesseract.image_to_string(thresh, config=custom_config).strip()\n",
    "            \n",
    "            # Clean up temp file\n",
    "            if os.path.exists(\"temp_captcha.png\"):\n",
    "                os.remove(\"temp_captcha.png\")\n",
    "            \n",
    "            # Basic validation\n",
    "            if len(captcha_text) >= 3 and captcha_text.replace(' ', '').isalnum():\n",
    "                print(f\"🤖 OCR solved captcha: {captcha_text}\")\n",
    "                return captcha_text.replace(' ', '')\n",
    "            else:\n",
    "                print(\"🤖 OCR failed, trying manual input...\")\n",
    "                return self.solve_captcha_manual(driver)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error solving captcha: {e}\")\n",
    "            return self.solve_captcha_manual(driver)\n",
    "    \n",
    "    def setup_driver(self) -> webdriver.Chrome:\n",
    "        \"\"\"Initialize Chrome WebDriver for Windows\"\"\"\n",
    "        try:\n",
    "            # Use ChromeDriverManager to automatically handle driver\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            driver = webdriver.Chrome(service=service, options=self.chrome_options)\n",
    "            return driver\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up Chrome driver: {e}\")\n",
    "            print(\"Please ensure Chrome browser is installed\")\n",
    "            raise\n",
    "    \n",
    "    def fill_form_and_submit(self, driver: webdriver.Chrome, from_date: str, to_date: str, max_retries: int = 3) -> bool:\n",
    "        \"\"\"Fill the judgment search form and submit\"\"\"\n",
    "        try:\n",
    "            # Wait for page to load - FIXED: Use until() instead of wait()\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            wait.until(EC.presence_of_element_located((By.NAME, \"fromDate\")))\n",
    "            \n",
    "            # Fill from date\n",
    "            from_date_field = driver.find_element(By.NAME, \"fromDate\")\n",
    "            from_date_field.clear()\n",
    "            from_date_field.send_keys(from_date)\n",
    "            \n",
    "            # Fill to date  \n",
    "            to_date_field = driver.find_element(By.NAME, \"toDate\")\n",
    "            to_date_field.clear()\n",
    "            to_date_field.send_keys(to_date)\n",
    "            \n",
    "            # Set reportable judgment to YES\n",
    "            try:\n",
    "                reportable_dropdown = Select(driver.find_element(By.NAME, \"reportable\"))\n",
    "                reportable_dropdown.select_by_value(\"Y\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not find reportable judgment dropdown: {e}\")\n",
    "            \n",
    "            # Handle captcha with retries\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    print(f\"🔍 Captcha attempt {attempt + 1}/{max_retries}\")\n",
    "                    \n",
    "                    captcha_text = self.solve_captcha(driver)\n",
    "                    \n",
    "                    if captcha_text:\n",
    "                        # Find captcha input field\n",
    "                        captcha_fields = driver.find_elements(By.XPATH, \"//input[contains(@name, 'captcha') or contains(@name, 'Captcha')]\")\n",
    "                        if not captcha_fields:\n",
    "                            captcha_fields = driver.find_elements(By.XPATH, \"//input[@type='text'][last()]\")\n",
    "                        \n",
    "                        if captcha_fields:\n",
    "                            captcha_field = captcha_fields[0]\n",
    "                            captcha_field.clear()\n",
    "                            captcha_field.send_keys(captcha_text)\n",
    "                            \n",
    "                            # Submit form\n",
    "                            submit_btns = driver.find_elements(By.XPATH, \"//input[@type='submit' or @value='Search' or @value='Submit']\")\n",
    "                            if submit_btns:\n",
    "                                submit_btns[0].click()\n",
    "                                \n",
    "                                # Wait and check if submission was successful\n",
    "                                time.sleep(3)\n",
    "                                page_source = driver.page_source.lower()\n",
    "                                \n",
    "                                if \"no records found\" not in page_source and \"invalid captcha\" not in page_source:\n",
    "                                    print(\"✅ Form submitted successfully!\")\n",
    "                                    return True\n",
    "                                else:\n",
    "                                    print(f\"❌ Captcha attempt {attempt + 1} failed, retrying...\")\n",
    "                            else:\n",
    "                                print(\"Could not find submit button\")\n",
    "                        else:\n",
    "                            print(\"Could not find captcha input field\")\n",
    "                    else:\n",
    "                        print(f\"❌ Could not solve captcha, attempt {attempt + 1}\")\n",
    "                    \n",
    "                    # Refresh page for retry\n",
    "                    if attempt < max_retries - 1:\n",
    "                        driver.refresh()\n",
    "                        time.sleep(2)\n",
    "                        wait.until(EC.presence_of_element_located((By.NAME, \"fromDate\")))\n",
    "                        \n",
    "                        # Refill the form\n",
    "                        from_date_field = driver.find_element(By.NAME, \"fromDate\")\n",
    "                        from_date_field.clear()\n",
    "                        from_date_field.send_keys(from_date)\n",
    "                        \n",
    "                        to_date_field = driver.find_element(By.NAME, \"toDate\")\n",
    "                        to_date_field.clear()\n",
    "                        to_date_field.send_keys(to_date)\n",
    "                        \n",
    "                        try:\n",
    "                            reportable_dropdown = Select(driver.find_element(By.NAME, \"reportable\"))\n",
    "                            reportable_dropdown.select_by_value(\"Y\")\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in captcha attempt {attempt + 1}: {e}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        driver.refresh()\n",
    "                        time.sleep(2)\n",
    "            \n",
    "            print(\"❌ Failed to solve captcha after all attempts\")\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error filling form: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_judgment_data(self, driver: webdriver.Chrome) -> List[Dict]:\n",
    "        \"\"\"Extract judgment data from results table\"\"\"\n",
    "        judgments = []\n",
    "        \n",
    "        try:\n",
    "            # Wait for results table\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "            \n",
    "            # Find the results table\n",
    "            tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "            results_table = None\n",
    "            \n",
    "            for table in tables:\n",
    "                table_text = table.text.lower()\n",
    "                if any(keyword in table_text for keyword in [\"s.no\", \"case\", \"judgment\", \"date\"]):\n",
    "                    results_table = table\n",
    "                    break\n",
    "            \n",
    "            if not results_table:\n",
    "                print(\"Could not find results table\")\n",
    "                return judgments\n",
    "            \n",
    "            # Extract table headers\n",
    "            headers = []\n",
    "            header_rows = results_table.find_elements(By.TAG_NAME, \"tr\")\n",
    "            if header_rows:\n",
    "                header_cells = header_rows[0].find_elements(By.TAG_NAME, \"th\")\n",
    "                if not header_cells:  # Try td if th not found\n",
    "                    header_cells = header_rows[0].find_elements(By.TAG_NAME, \"td\")\n",
    "                \n",
    "                for cell in header_cells:\n",
    "                    headers.append(cell.text.strip())\n",
    "            \n",
    "            if not headers:\n",
    "                headers = [f\"Column_{i+1}\" for i in range(10)]  # Default headers\n",
    "            \n",
    "            print(f\"📊 Found table with headers: {headers}\")\n",
    "            \n",
    "            # Extract data rows\n",
    "            rows = results_table.find_elements(By.TAG_NAME, \"tr\")[1:]  # Skip header\n",
    "            \n",
    "            for i, row in enumerate(rows):\n",
    "                try:\n",
    "                    cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                    if len(cells) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    judgment_data = {}\n",
    "                    for j, cell in enumerate(cells):\n",
    "                        header_name = headers[j] if j < len(headers) else f\"Column_{j+1}\"\n",
    "                        judgment_data[header_name] = cell.text.strip()\n",
    "                    \n",
    "                    # Look for PDF download link\n",
    "                    pdf_links = row.find_elements(By.XPATH, \".//a[contains(@href, '.pdf') or contains(text(), 'View') or contains(text(), 'Download')]\")\n",
    "                    if pdf_links:\n",
    "                        judgment_data['pdf_url'] = pdf_links[0].get_attribute('href')\n",
    "                    else:\n",
    "                        judgment_data['pdf_url'] = \"\"\n",
    "                    \n",
    "                    if judgment_data:  # Only add if we got some data\n",
    "                        judgments.append(judgment_data)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row {i}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"📋 Extracted {len(judgments)} judgment records\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting judgment data: {e}\")\n",
    "        \n",
    "        return judgments\n",
    "    \n",
    "    def download_pdf(self, url: str, filename: str) -> bool:\n",
    "        \"\"\"Download PDF from URL\"\"\"\n",
    "        try:\n",
    "            print(f\"📥 Downloading: {filename}\")\n",
    "            response = requests.get(url, timeout=30, headers={\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            })\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            pdf_path = self.pdf_dir / filename\n",
    "            with open(pdf_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error downloading PDF {filename}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_pdf_filename(self, judgment_data: Dict) -> str:\n",
    "        \"\"\"Generate safe filename for PDF\"\"\"\n",
    "        # Try to find case number in any column\n",
    "        case_num = \"Unknown\"\n",
    "        date_str = \"Unknown\"\n",
    "        \n",
    "        for key, value in judgment_data.items():\n",
    "            if \"case\" in key.lower() and value:\n",
    "                case_num = str(value).replace('/', '_').replace('\\\\', '_')[:50]\n",
    "            elif \"date\" in key.lower() and value:\n",
    "                date_str = str(value).replace('/', '_').replace('-', '_')[:20]\n",
    "        \n",
    "        # Create safe filename\n",
    "        safe_filename = f\"{case_num}_{date_str}.pdf\"\n",
    "        safe_filename = \"\".join(c for c in safe_filename if c.isalnum() or c in \"._-\")\n",
    "        \n",
    "        return safe_filename\n",
    "    \n",
    "    def scrape_judgments(self, from_date: str, to_date: str) -> List[Dict]:\n",
    "        \"\"\"Main scraping function\"\"\"\n",
    "        print(f\"🔍 Scraping judgments from {from_date} to {to_date}\")\n",
    "        \n",
    "        try:\n",
    "            driver = self.setup_driver()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to setup driver: {e}\")\n",
    "            return []\n",
    "        \n",
    "        all_judgments = []\n",
    "        \n",
    "        try:\n",
    "            print(\"🌐 Loading website...\")\n",
    "            driver.get(self.base_url)\n",
    "            \n",
    "            if self.fill_form_and_submit(driver, from_date, to_date):\n",
    "                print(\"📊 Extracting judgment data...\")\n",
    "                judgments = self.extract_judgment_data(driver)\n",
    "                \n",
    "                for judgment in judgments:\n",
    "                    judgment_id = self.generate_judgment_id(judgment)\n",
    "                    \n",
    "                    # Skip if already downloaded\n",
    "                    if judgment_id in self.downloaded_judgments[\"downloaded_ids\"]:\n",
    "                        case_ref = list(judgment.values())[0] if judgment else \"Unknown\"\n",
    "                        print(f\"⏭️ Skipping already downloaded: {case_ref}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Download PDF if URL exists\n",
    "                    pdf_filename = \"\"\n",
    "                    if judgment.get('pdf_url'):\n",
    "                        pdf_filename = self.generate_pdf_filename(judgment)\n",
    "                        if self.download_pdf(judgment['pdf_url'], pdf_filename):\n",
    "                            judgment['pdf_filename'] = pdf_filename\n",
    "                            print(f\"✅ Downloaded: {pdf_filename}\")\n",
    "                        else:\n",
    "                            judgment['pdf_filename'] = \"Download_Failed\"\n",
    "                    else:\n",
    "                        judgment['pdf_filename'] = \"No_PDF_URL\"\n",
    "                    \n",
    "                    # Mark as downloaded\n",
    "                    self.downloaded_judgments[\"downloaded_ids\"].add(judgment_id)\n",
    "                    all_judgments.append(judgment)\n",
    "                \n",
    "            else:\n",
    "                print(\"❌ Failed to submit form\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during scraping: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return all_judgments\n",
    "    \n",
    "    def save_to_csv(self, judgments: List[Dict]):\n",
    "        \"\"\"Save judgments to CSV file\"\"\"\n",
    "        if not judgments:\n",
    "            print(\"ℹ️ No new judgments to save\")\n",
    "            return\n",
    "        \n",
    "        # Load existing data if CSV exists\n",
    "        existing_df = pd.DataFrame()\n",
    "        if self.csv_file.exists():\n",
    "            try:\n",
    "                existing_df = pd.read_csv(self.csv_file)\n",
    "                print(f\"📂 Loaded {len(existing_df)} existing records\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load existing CSV: {e}\")\n",
    "        \n",
    "        # Create new DataFrame\n",
    "        new_df = pd.DataFrame(judgments)\n",
    "        \n",
    "        # Combine and save\n",
    "        if not existing_df.empty:\n",
    "            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        else:\n",
    "            combined_df = new_df\n",
    "        \n",
    "        combined_df.to_csv(self.csv_file, index=False)\n",
    "        print(f\"💾 Saved {len(judgments)} new judgments to {self.csv_file}\")\n",
    "    \n",
    "    def run_incremental_scrape(self):\n",
    "        \"\"\"Run incremental scraping for last 10 days\"\"\"\n",
    "        today = datetime.now()\n",
    "        from_date_obj = today - timedelta(days=10)\n",
    "        \n",
    "        from_date = from_date_obj.strftime(\"%d/%m/%Y\")\n",
    "        to_date = today.strftime(\"%d/%m/%Y\")\n",
    "        \n",
    "        print(f\"📅 Running incremental scrape from {from_date} to {to_date}\")\n",
    "        \n",
    "        judgments = self.scrape_judgments(from_date, to_date)\n",
    "        \n",
    "        if judgments:\n",
    "            self.save_to_csv(judgments)\n",
    "        \n",
    "        # Update state\n",
    "        self.downloaded_judgments[\"last_run_date\"] = today.isoformat()\n",
    "        self.save_state()\n",
    "        \n",
    "        print(f\"✅ Scraping completed. Downloaded {len(judgments)} new judgments.\")\n",
    "        return judgments\n",
    "\n",
    "# Windows-specific helper functions\n",
    "def show_file_structure():\n",
    "    \"\"\"Show file structure using Windows commands\"\"\"\n",
    "    import glob\n",
    "    \n",
    "    base_dir = \"rajasthan_hc_judgments\"\n",
    "    if os.path.exists(base_dir):\n",
    "        print(f\"\\n📁 Files in {base_dir}:\")\n",
    "        for file in glob.glob(f\"{base_dir}/*\"):\n",
    "            size = os.path.getsize(file) if os.path.isfile(file) else 0\n",
    "            file_type = \"📄\" if os.path.isfile(file) else \"📁\"\n",
    "            print(f\"  {file_type} {os.path.basename(file)} ({size} bytes)\")\n",
    "        \n",
    "        pdf_dir = f\"{base_dir}/pdfs\"\n",
    "        if os.path.exists(pdf_dir):\n",
    "            pdf_files = glob.glob(f\"{pdf_dir}/*.pdf\")\n",
    "            print(f\"\\n📚 PDFs downloaded ({len(pdf_files)} files):\")\n",
    "            for pdf in pdf_files[:5]:  # Show first 5\n",
    "                size = os.path.getsize(pdf)\n",
    "                print(f\"  📄 {os.path.basename(pdf)} ({size} bytes)\")\n",
    "            if len(pdf_files) > 5:\n",
    "                print(f\"  ... and {len(pdf_files) - 5} more PDFs\")\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    \"\"\"Main function for Windows\"\"\"\n",
    "    print(\"🏛️ Rajasthan High Court Judgment Scraper (Windows Version)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if tesseract is available\n",
    "    if not TESSERACT_AVAILABLE:\n",
    "        print(\"⚠️ Note: Tesseract OCR not found. You'll need to enter captchas manually.\")\n",
    "        print(\"📥 To install Tesseract: https://github.com/UB-Mannheim/tesseract/wiki\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize scraper\n",
    "        print(\"🚀 Initializing scraper...\")\n",
    "        scraper = RajasthanHCJudgmentScraperWindows()\n",
    "        \n",
    "        # Run incremental scrape\n",
    "        print(\"📥 Running incremental scrape for last 10 days...\")\n",
    "        judgments = scraper.run_incremental_scrape()\n",
    "        \n",
    "        if judgments:\n",
    "            print(f\"✅ Successfully processed {len(judgments)} judgments\")\n",
    "            \n",
    "            # Show results\n",
    "            if scraper.csv_file.exists():\n",
    "                df = pd.read_csv(scraper.csv_file)\n",
    "                print(f\"\\n📊 Total judgments in database: {len(df)}\")\n",
    "                print(\"\\n📋 Column names:\", list(df.columns))\n",
    "                if len(df) > 0:\n",
    "                    print(\"\\n🔍 Sample data:\")\n",
    "                    print(df.head(2).to_string())\n",
    "        else:\n",
    "            print(\"ℹ️ No new judgments found\")\n",
    "        \n",
    "        # Show file structure\n",
    "        show_file_structure()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        print(\"\\n🔧 Troubleshooting tips:\")\n",
    "        print(\"1. Make sure Chrome browser is installed\")\n",
    "        print(\"2. Check your internet connection\")\n",
    "        print(\"3. The website might be temporarily unavailable\")\n",
    "        print(\"4. Try running with manual captcha input\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24fc309",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (My Project)",
   "language": "python",
   "name": "myprojectenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
